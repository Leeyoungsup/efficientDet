{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'efficientdet_d0'\n",
    "\n",
    "NUM_CLASSES = 80\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "INITIAL_LR = 0.01\n",
    "DECAY_STEPS = 433 * 155\n",
    "width=640\n",
    "height=512\n",
    "train_data=pd.read_csv('../../data/txt/train.csv',  header=None)\n",
    "image_path='../../data/image/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anchors():\n",
    "    \"\"\"Anchor boxes generator.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 aspect_ratios=[0.5, 1, 2],\n",
    "                 scales=[0, 1/3, 2/3]):\n",
    "        \"\"\"Initialize anchors generator.\n",
    "\n",
    "        Args:\n",
    "            aspect_ratios: a list of floats representing aspect\n",
    "                ratios of anchor boxes on each feature level.\n",
    "            scales: a list of floats representing different scales\n",
    "                of anchor boxes on each feature level.\n",
    "        \"\"\"\n",
    "        self._aspect_ratios = aspect_ratios\n",
    "        self._scales = [2**i for i in scales]\n",
    "        self._num_anchors = len(aspect_ratios) * len(scales)\n",
    "\n",
    "        self._strides = [2**i for i in range(3, 8)]\n",
    "        self._areas = [i**2 for i in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Compute height and width for each anchor box on each level.\n",
    "\n",
    "        Returns:\n",
    "            A float tensor with shape (5, num_anchors, 2) where each\n",
    "                pair representing height and width of anchor box.\n",
    "        \"\"\"\n",
    "        all_dims = list()\n",
    "        for area in self._areas:\n",
    "            level_dims = list()\n",
    "            for aspect_ratio in self._aspect_ratios:\n",
    "                height = tf.math.sqrt(area * aspect_ratio)\n",
    "                width = area / height\n",
    "                dims = tf.cast([height, width], tf.float32)\n",
    "                for scale in self._scales:\n",
    "                    level_dims.append(dims * scale)\n",
    "            all_dims.append(tf.stack(level_dims, axis=0))\n",
    "        return tf.stack(all_dims, axis=0)\n",
    "\n",
    "    @tf.function\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Get anchors for with given height and width on given level.\n",
    "\n",
    "        Args:\n",
    "            feature_height: an integer representing height of feature map.\n",
    "                Should be divisible by 2**level.\n",
    "            feature_width: an integer representing width of feature map.\n",
    "                Should be divisible by 2**level.\n",
    "            level: an integer from range [3, 7] representing level\n",
    "                of feature map.\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + .5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + .5\n",
    "        xs = tf.tile(tf.reshape(rx, [1, -1]), [tf.shape(ry)[0], 1])\n",
    "        ys = tf.tile(tf.reshape(ry, [-1, 1]), [1, tf.shape(rx)[0]])\n",
    "\n",
    "        centers = tf.stack([xs, ys], axis=-1) * self._strides[level - 3]\n",
    "        centers = tf.reshape(centers, [-1, 1, 2])\n",
    "        centers = tf.tile(centers, [1, self._num_anchors, 1])\n",
    "        centers = tf.reshape(centers, [-1, 2])\n",
    "\n",
    "        dims = tf.tile(self._anchor_dims[level - 3], [feature_height * feature_width, 1])\n",
    "        return tf.concat([centers, dims], axis=-1)\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Get anchors for given height and width on all levels.\n",
    "\n",
    "        Args:\n",
    "            image_height: an integer representing height of image.\n",
    "            image_width: an integer representing width of image.\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2**i),\n",
    "                tf.math.ceil(image_width / 2**i),\n",
    "                i\n",
    "            ) for i in range(3, 8)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "\n",
    "def to_corners(bbox):\n",
    "    \"\"\"Convert [x, y, width, height] to [x_min, y_min, x_max, y_max].\"\"\"\n",
    "    return tf.concat(\n",
    "        [bbox[..., :2] - bbox[..., 2:] / 2.0, bbox[..., :2] + bbox[..., 2:] / 2.0], axis=-1\n",
    "    )\n",
    "def compute_iou(boxes_1, boxes_2):\n",
    "    \"\"\"Compute intersection over union.\n",
    "\n",
    "    Args:\n",
    "        boxes_1: a tensor with shape (N, 4) representing bounding boxes\n",
    "            where each box is of the format [x, y, width, height].\n",
    "        boxes_2: a tensor with shape (M, 4) representing bounding boxes\n",
    "            where each box is of the format [x, y, width, height].\n",
    "\n",
    "    Returns:\n",
    "        IOU matrix with shape (N, M).\n",
    "    \"\"\"\n",
    "\n",
    "    boxes_1_corners = to_corners(boxes_1)\n",
    "    boxes_2_corners = to_corners(boxes_2)\n",
    "\n",
    "    left_upper = tf.maximum(boxes_1_corners[..., None, :2], boxes_2_corners[..., :2])\n",
    "    right_lower = tf.minimum(boxes_1_corners[..., None, 2:], boxes_2_corners[..., 2:])\n",
    "    diff = tf.maximum(0.0, right_lower - left_upper)\n",
    "    intersection = diff[..., 0] * diff[..., 1]\n",
    "\n",
    "    boxes_1_area = boxes_1[..., 2] * boxes_1[..., 3]\n",
    "    boxes_2_area = boxes_2[..., 2] * boxes_2[..., 3]\n",
    "    union = boxes_1_area[..., None] + boxes_2_area - intersection\n",
    "\n",
    "    iou = intersection / union\n",
    "    return tf.clip_by_value(iou, 0.0, 1.0)\n",
    "\n",
    "class SamplesEncoder():\n",
    "    \"\"\"Enchoder of training batches.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 aspect_ratios=[0.5, 1, 2],\n",
    "                 scales=[0, 1/3, 2/3]):\n",
    "        self._anchors = Anchors()\n",
    "        self._box_variance = tf.cast(\n",
    "            [0.1, 0.1, 0.2, 0.2], tf.float32\n",
    "        )\n",
    "\n",
    "    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4):\n",
    "        \"\"\"Assign ground truth boxes to all anchor boxes.\"\"\"\n",
    "\n",
    "        iou = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou, axis=1)\n",
    "        matched_gt_idx = tf.argmax(iou, axis=1)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, classes):\n",
    "        anchor_boxes = self._anchors.get_anchors(image_shape[1], image_shape[2])\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "\n",
    "        classes = tf.cast(classes, dtype=tf.float32)\n",
    "        matched_gt_classes = tf.gather(classes, matched_gt_idx)\n",
    "        class_target = tf.where(tf.equal(positive_mask, 1.0), matched_gt_classes, -1.0)\n",
    "        class_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, class_target)\n",
    "        class_target = tf.expand_dims(class_target, axis=-1)\n",
    "\n",
    "        label = tf.concat([box_target, class_target], axis=-1)\n",
    "\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, images, gt_boxes, classes):\n",
    "        \"\"\"Encode batch for training.\"\"\"\n",
    "\n",
    "        images_shape = tf.shape(images)\n",
    "        batch_size = images_shape[0]\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], classes[i])\n",
    "            labels = labels.write(i, label)\n",
    "        images = tf.keras.applications.efficientnet.preprocess_input(images)\n",
    "        return images, labels.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_train_data=train_data.loc[train_data[5] == 'cancer'].reset_index()\n",
    "train_image=np.zeros((len(cancer_train_data),height,width,3))\n",
    "train_label=np.zeros((len(cancer_train_data),4))\n",
    "train_class=np.zeros((len(cancer_train_data),1))\n",
    "for i in range(len(cancer_train_data)):\n",
    "    img=Image.open(image_path+os.path.basename(cancer_train_data[0][i])).resize([width,height])\n",
    "    train_image[i]=np.array(img)\n",
    "    train_label[i]=np.array((cancer_train_data[1][i],cancer_train_data[2][i],cancer_train_data[3][i],cancer_train_data[4][i]))//2\n",
    "    train_class[i]=0\n",
    "# samples_encoder = SamplesEncoder()\n",
    "# autotune = tf.data.experimental.AUTOTUNE\n",
    "# train_data = train_data.shuffle(5000)\n",
    "# train_data = train_data.padded_batch(BATCH_SIZE, padding_values=(0.0, 1e-8, -1.0))\n",
    "# train_data = train_data.map(samples_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "# train_data = train_data.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 225., 225., 419.],\n",
       "       [  0.,  50., 128., 165.],\n",
       "       [ 15.,  43., 110., 138.],\n",
       "       ...,\n",
       "       [113.,  93., 381., 317.],\n",
       "       [114.,  60., 219., 190.],\n",
       "       [ 99.,  56., 175., 174.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(name='efficientnet_b0',\n",
    "                 weights='imagenet'):\n",
    "\n",
    "    models = {\n",
    "        'efficientnet_b0': tf.keras.applications.EfficientNetB0,\n",
    "        'efficientnet_b1': tf.keras.applications.EfficientNetB1,\n",
    "        'efficientnet_b2': tf.keras.applications.EfficientNetB2,\n",
    "        'efficientnet_b3': tf.keras.applications.EfficientNetB3,\n",
    "        'efficientnet_b4': tf.keras.applications.EfficientNetB4,\n",
    "        'efficientnet_b5': tf.keras.applications.EfficientNetB5,\n",
    "        'efficientnet_b6': tf.keras.applications.EfficientNetB6,\n",
    "        'efficientnet_b7': tf.keras.applications.EfficientNetB7\n",
    "    }\n",
    "\n",
    "    backbone = models[name](include_top=False,\n",
    "                            weights=weights,\n",
    "                            input_shape=[None, None, 3])\n",
    "\n",
    "    outputs = [backbone.get_layer(layer_name).output for layer_name in [\n",
    "        'block3b_add', # LEVEL P3\n",
    "        'block5c_add', # LEVEL P4\n",
    "        'top_activation' # LEVEL P5\n",
    "        ]]\n",
    "\n",
    "    return tf.keras.Model(inputs=backbone.inputs, outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiFPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiFPNLayerNode(tf.keras.layers.Layer):\n",
    "    \"\"\"One node in BiFPN for features fusing.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels=64,\n",
    "                 kernel_size=3,\n",
    "                 depth_multiplier=1,\n",
    "                 name='BiFPN_node'):\n",
    "        \"\"\"Ininitialize node.\n",
    "        Args:\n",
    "            channels: an integer representing number of units inside the node.\n",
    "            kernel_size: an integer or tuple/list of 2 integers, specifying \n",
    "                the height and width of the 2D convolution window.\n",
    "            depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layer.\n",
    "            name: a string representing layer name.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.channels = channels\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def build(self, inputs):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(len(inputs), self.channels),\n",
    "            initializer=\"ones\",\n",
    "            name='sum_weights',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.conv2d = tf.keras.layers.SeparableConv2D(\n",
    "            self.channels,\n",
    "            self.kernel_size,\n",
    "            padding='same',\n",
    "            depth_multiplier=self.depth_multiplier,\n",
    "            pointwise_initializer=tf.initializers.variance_scaling(),\n",
    "            depthwise_initializer=tf.initializers.variance_scaling(),\n",
    "            name='node_conv'\n",
    "        )\n",
    "\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.act = tf.keras.layers.Activation(tf.nn.silu)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Fuse features.\n",
    "        Args:\n",
    "            inputs: a list with length equal to self.w.shape[0] of feature maps\n",
    "                with equal shapes.\n",
    "        Returns:\n",
    "            A float tensor of fused features after applying convolution\n",
    "            with batch normalization and SiLU activation.\n",
    "        \"\"\"\n",
    "        norm = tf.math.reduce_sum(self.w, axis=0) + 1e-4\n",
    "        scaled_tensors = [inputs[i] * self.w[i] / norm for i in range(self.w.shape[0])]\n",
    "        w_sum = tf.math.add_n(scaled_tensors)\n",
    "        conv = self.conv2d(w_sum)\n",
    "        bn = self.bn(conv, training=training)\n",
    "        return self.act(bn)\n",
    "class BiFPNLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"One layer of BiFPN.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels=64,\n",
    "                 kernel_size=3,\n",
    "                 depth_multiplier=1,\n",
    "                 pooling_strategy='avg',\n",
    "                 name='BiFPN_Layer'):\n",
    "        \"\"\"Initialize BiFPN layer.\n",
    "        Args:\n",
    "            channels: an integer representing number of units inside each fusing node.\n",
    "            kernel_size: an integer or tuple/list of 2 integers, specifying \n",
    "                the height and width of the 2D convolution window.\n",
    "            depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers in BiFPN nodes.\n",
    "            pooling_strategy: a string representing pooling strategy.\n",
    "                'avg' or 'max'. Otherwise the max pooling will be selected.\n",
    "            name: a string representing layer name.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "\n",
    "        self.first_step_nodes = [BiFPNLayerNode(channels=channels,\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                depth_multiplier=depth_multiplier,\n",
    "                                                name=f'step_1_level_{i}_node') for i in range(4, 7)]\n",
    "        self.second_step_nodes = [BiFPNLayerNode(channels=channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 depth_multiplier=depth_multiplier,\n",
    "                                                 name=f'step_2_level_{i}_node') for i in range(3, 8)]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Perfrom features fusing from different levels. (Inputs length equals 5)\"\"\"\n",
    "\n",
    "        # TOP-DOWN PATHWAY\n",
    "        # UPSAMPLE LEVEL 7 FEATURE MAP\n",
    "        upscaled = self._upscale2d(inputs[-1])\n",
    "        # FUSE LEVELS 6 AND 7\n",
    "        first_step_outs = [self.first_step_nodes[-1]([inputs[-2], upscaled], training=training)]\n",
    "        for i in range(2):\n",
    "            # UPSAMPLE PREVIOUS RESULT OF FUSING\n",
    "            upscaled = self._upscale2d(first_step_outs[i])\n",
    "            # FUSE FEATURE MAPS\n",
    "            fused = self.first_step_nodes[1-i]([inputs[-3-i], upscaled])\n",
    "            first_step_outs.append(fused)\n",
    "\n",
    "        # BOTTOM-UP PATHWAY\n",
    "        # UPSAMPLE LAST RESULT OF FEATURE FUSING FROM TOP-DOWN PATH   \n",
    "        upscaled = self._upscale2d(first_step_outs[-1])\n",
    "        # FUSE LEVELS 3 AND 4^TD\n",
    "        second_step_outs = [self.second_step_nodes[0]([inputs[0], upscaled])]\n",
    "        for i in range(1, 4):\n",
    "            # DOWNSAMPLE PREVIOUS RESULT OF FUSING\n",
    "            downscaled = self._pool2d(second_step_outs[-1])\n",
    "            # FUSE FEATURES\n",
    "            fused = self.second_step_nodes[i]([inputs[i], first_step_outs[3-i], downscaled], training=training)\n",
    "            second_step_outs.append(fused)\n",
    "        downscaled = self._pool2d(second_step_outs[-1])\n",
    "        # FUSE LEVELS 7 AND 6^OUT\n",
    "        fused = self.second_step_nodes[-1]([inputs[-1], downscaled])\n",
    "        second_step_outs.append(fused)\n",
    "\n",
    "        return second_step_outs\n",
    "\n",
    "    def _pool2d(self, inputs):\n",
    "        if self.pooling_strategy == 'avg':\n",
    "            return tf.keras.layers.AveragePooling2D()(inputs)\n",
    "        else:\n",
    "            return tf.keras.layers.MaxPool2D()(inputs)\n",
    "\n",
    "    def _upscale2d(self, inputs):\n",
    "        return tf.keras.layers.UpSampling2D()(inputs)\n",
    "class BiFPN(tf.keras.layers.Layer):\n",
    "    \"\"\"Bidirectional Feature Pyramid Network.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels=64,\n",
    "                 depth=3,\n",
    "                 kernel_size=3,\n",
    "                 depth_multiplier=1,\n",
    "                 pooling_strategy='avg',\n",
    "                 name='BiFPN'):\n",
    "        super().__init__(name=name)\n",
    "        \"\"\"Initialize BiFPN.\n",
    "        Args:\n",
    "            channels: an integer representing number of units inside each fusing node\n",
    "                and convolution layer.\n",
    "            depth: an integer representing number of BiFPN layers. depth > 0.\n",
    "            kernel_size: an integer or tuple/list of 2 integers, specifying \n",
    "                the height and width of the 2D convolution window.\n",
    "            depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers in BiFPN nodes.\n",
    "            pooling_strategy: a string representing pooling strategy in BiFPN layers.\n",
    "                'avg' or 'max'. Otherwise the max pooling will be selected.\n",
    "            name: a string representing layer name.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "\n",
    "        self.convs_1x1 = [tf.keras.layers.Conv2D(channels,\n",
    "                                                 1,\n",
    "                                                 padding='same',\n",
    "                                                 name=f'1x1_conv_level_{3+i}') for i in range(5)]\n",
    "\n",
    "        self.bns = [\n",
    "            tf.keras.layers.BatchNormalization(name=f'bn_level_{i}') for i in range(5)\n",
    "        ]\n",
    "        self.act = tf.keras.layers.Activation(tf.nn.silu)\n",
    "\n",
    "        self.bifpn_layers = [BiFPNLayer(channels=channels,\n",
    "                                        kernel_size=kernel_size,\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        pooling_strategy=pooling_strategy,\n",
    "                                        name=f'BiFPN_Layer_{i}') for i in range(depth)]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        assert len(inputs) == 5\n",
    "\n",
    "        squeezed = [self.convs_1x1[i](inputs[i]) for i in range(5)]\n",
    "        normalized = [self.bns[i](squeezed[i], training=training) for i in range(5)]\n",
    "        activated = [self.act(normalized[i]) for i in range(5)]\n",
    "        feature_maps = self.bifpn_layers[0](activated, training=training)\n",
    "        for layer in self.bifpn_layers[1:]:\n",
    "            feature_maps = layer(feature_maps, training=training)\n",
    "\n",
    "        return feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal loss implementations.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha=0.25,\n",
    "                 gamma=1.5,\n",
    "                 label_smoothing=0.1,\n",
    "                 name='focal_loss'):\n",
    "        \"\"\"Initialize parameters for Focal loss.\n",
    "\n",
    "        FL = - alpha_t * (1 - p_t) ** gamma * log(p_t)\n",
    "        This implementation also includes label smoothing for preventing overconfidence.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name, reduction=\"none\")\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Focal loss.\n",
    "\n",
    "        Args:\n",
    "            y_true: a tensor of ground truth values with\n",
    "                shape (batch_size, num_anchor_boxes, num_classes).\n",
    "            y_pred: a tensor of predicted values with\n",
    "                shape (batch_size, num_anchor_boxes, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            A float tensor with shape (batch_size, num_anchor_boxes) with\n",
    "            loss value for every anchor box.\n",
    "        \"\"\"\n",
    "        prob = tf.sigmoid(y_pred)\n",
    "        pt = y_true * prob + (1 - y_true) * (1 - prob)\n",
    "        at = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "\n",
    "        y_true = y_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "        loss = at * (1.0 - pt)**self.gamma * ce\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class BoxLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Huber loss implementation.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 delta=1.0,\n",
    "                 name='box_loss'):\n",
    "        super().__init__(name=name, reduction=\"none\")\n",
    "        self.delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Huber loss.\n",
    "\n",
    "        Args:\n",
    "            y_true: a tensor of ground truth values with shape (batch_size, num_anchor_boxes, 4).\n",
    "            y_pred: a tensor of predicted values with shape (batch_size, num_anchor_boxes, 4).\n",
    "\n",
    "        Returns:\n",
    "            A float tensor with shape (batch_size, num_anchor_boxes) with\n",
    "            loss value for every anchor box.\n",
    "        \"\"\"\n",
    "        loss = tf.abs(y_true - y_pred)\n",
    "        l1 = self.delta * (loss - 0.5 * self.delta)\n",
    "        l2 = 0.5 * loss ** 2\n",
    "        box_loss = tf.where(tf.less(loss, self.delta), l2, l1)\n",
    "        return tf.reduce_sum(box_loss, axis=-1)\n",
    "\n",
    "\n",
    "class EffDetLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Composition of Focal and Huber losses.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes=80,\n",
    "                 alpha=0.25,\n",
    "                 gamma=1.5,\n",
    "                 label_smoothing=0.1,\n",
    "                 delta=1.0,\n",
    "                 name='effdet_loss'):\n",
    "        \"\"\"Initialize Focal and Huber loss.\n",
    "\n",
    "        Args:\n",
    "            num_classes: an integer number representing number of\n",
    "                all possible classes in training dataset.\n",
    "            alpha: a float number for Focal loss formula.\n",
    "            gamma: a float number for Focal loss formula.\n",
    "            label_smoothing: a float number of label smoothing intensity.\n",
    "            delta: a float number representing a threshold in Huber loss\n",
    "                for choosing between linear and cubic loss.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.class_loss = FocalLoss(alpha=alpha, gamma=gamma, label_smoothing=label_smoothing)\n",
    "        self.box_loss = BoxLoss(delta=delta)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Focal and Huber losses for every anchor box.\n",
    "\n",
    "        Args:\n",
    "            y_true: a tensor of ground truth values with shape (batch_size, num_anchor_boxes, 5)\n",
    "                representing anchor box correction and class label.\n",
    "            y_pred: a tensor of predicted values with\n",
    "                shape (batch_size, num_anchor_boxes, num_classes).\n",
    "\n",
    "        Returns:\n",
    "            loss: a float loss value.\n",
    "        \"\"\"\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "        box_labels = y_true[..., :4]\n",
    "        box_preds = y_pred[..., :4]\n",
    "\n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[..., 4], dtype=tf.int32),\n",
    "            depth=self.num_classes,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        cls_preds = y_pred[..., 4:]\n",
    "\n",
    "        positive_mask = tf.cast(tf.greater(y_true[..., 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[..., 4], -2.0), dtype=tf.float32)\n",
    "\n",
    "        clf_loss = self.class_loss(cls_labels, cls_preds)\n",
    "        box_loss = self.box_loss(box_labels, box_preds)\n",
    "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        loss = clf_loss + box_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDetector(tf.keras.layers.Layer):\n",
    "    \"\"\"Classification head.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes=80,\n",
    "                 channels=64,\n",
    "                 num_anchors=9,\n",
    "                 depth=3,\n",
    "                 kernel_size=3,\n",
    "                 depth_multiplier=1,\n",
    "                 name='class_det'):\n",
    "        \"\"\"Initialize classification model.\n",
    "        Args:\n",
    "            num_classes: an integer representing number of classes\n",
    "                to predict.\n",
    "            channels: an integer representing number of filters\n",
    "                inside each separable convolution layer.\n",
    "            num_anchors: an integer representing number of anchor\n",
    "                boxes.\n",
    "            depth: an integer representing number of separable\n",
    "                convolutions before final convolution.\n",
    "            kernel_size: an integer or tuple/list of 2 integers, specifying \n",
    "                the height and width of the 2D convolution window.\n",
    "            depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers.\n",
    "            name: a string representing layer name.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.num_classes = num_classes\n",
    "        self.channels = channels\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "\n",
    "        self.convs = [tf.keras.layers.SeparableConv2D(\n",
    "            channels,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            depth_multiplier=depth_multiplier,\n",
    "            pointwise_initializer=tf.initializers.variance_scaling(),\n",
    "            depthwise_initializer=tf.initializers.variance_scaling(),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            name=f'class_det_separable_conv_{i}'\n",
    "        ) for i in range(depth)]\n",
    "\n",
    "        self.bns = [\n",
    "            tf.keras.layers.BatchNormalization(name=f'bn_{i}') for i in range(depth)\n",
    "        ]\n",
    "        self.act = tf.keras.layers.Activation(tf.nn.silu)\n",
    "\n",
    "        bias_init = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.classes = tf.keras.layers.SeparableConv2D(\n",
    "            num_classes * num_anchors,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            depth_multiplier=depth_multiplier,\n",
    "            activation=None,\n",
    "            pointwise_initializer=tf.initializers.variance_scaling(),\n",
    "            depthwise_initializer=tf.initializers.variance_scaling(),\n",
    "            bias_initializer=bias_init,\n",
    "            name='class_preds'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        for i in range(self.depth):\n",
    "            inputs = self.convs[i](inputs)\n",
    "            inputs = self.bns[i](inputs, training=training)\n",
    "            inputs = self.act(inputs)\n",
    "        class_output = self.classes(inputs)\n",
    "\n",
    "        return class_output\n",
    "\n",
    "\n",
    "class BoxRegressor(tf.keras.layers.Layer):\n",
    "    \"\"\"Regression head.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels=64,\n",
    "                 num_anchors=9,\n",
    "                 depth=3,\n",
    "                 kernel_size=3,\n",
    "                 depth_multiplier=1,\n",
    "                 name='box_regressor'):\n",
    "        \"\"\"Initialize regression model.\n",
    "        Args:\n",
    "            channels: an integer representing number of filters\n",
    "                inside each separable convolution layer.\n",
    "            num_anchors: an integer representing number of anchor\n",
    "                boxes.\n",
    "            depth: an integer representing number of separable\n",
    "                convolutions before final convolution.\n",
    "            kernel_size: an integer or tuple/list of 2 integers, specifying\n",
    "                the height and width of the 2D convolution window.\n",
    "            depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers.\n",
    "            name: a string representing layer name.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.channels=channels\n",
    "        self.num_anchors=num_anchors\n",
    "        self.depth=depth\n",
    "        self.kernel_size=kernel_size\n",
    "        self.depth_multiplier=depth_multiplier\n",
    "\n",
    "        self.convs = [tf.keras.layers.SeparableConv2D(\n",
    "            channels,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            depth_multiplier=depth_multiplier,\n",
    "            pointwise_initializer=tf.initializers.variance_scaling(),\n",
    "            depthwise_initializer=tf.initializers.variance_scaling(),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            name=f'box_reg_separable_conv_{i}'\n",
    "        ) for i in range(depth)]\n",
    "\n",
    "        self.bns = [\n",
    "            tf.keras.layers.BatchNormalization(name=f'bn_{i}') for i in range(depth)\n",
    "        ]\n",
    "        self.act = tf.keras.layers.Activation(tf.nn.silu)\n",
    "\n",
    "        self.boxes = tf.keras.layers.SeparableConv2D(\n",
    "            4 * num_anchors,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            depth_multiplier=depth_multiplier,\n",
    "            activation=None,\n",
    "            pointwise_initializer=tf.initializers.variance_scaling(),\n",
    "            depthwise_initializer=tf.initializers.variance_scaling(),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            name='box_preds'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        for i in range(self.depth):\n",
    "            inputs = self.convs[i](inputs)\n",
    "            inputs = self.bns[i](inputs, training=training)\n",
    "            inputs = self.act(inputs)\n",
    "        box_output = self.boxes(inputs)\n",
    "\n",
    "        return box_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientDet(tf.keras.Model):\n",
    "    \"\"\"EfficientDet model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels=64,\n",
    "                 num_classes=80,\n",
    "                 num_anchors=9,\n",
    "                 bifpn_depth=3,\n",
    "                 bifpn_kernel_size=3,\n",
    "                 bifpn_depth_multiplier=1,\n",
    "                 bifpn_pooling_strategy='avg',\n",
    "                 heads_depth=3,\n",
    "                 class_kernel_size=3,\n",
    "                 class_depth_multiplier=1,\n",
    "                 box_kernel_size=3,\n",
    "                 box_depth_multiplier=1,\n",
    "                 backbone_name='efficientnet_b0',\n",
    "                 name='efficientdet_d0'):\n",
    "        \"\"\"Initialize EffDet. Default args refers to EfficientDet D0.\n",
    "        Args:\n",
    "            channels: an integer representing number of units inside each fusing\n",
    "                node and convolution layer of BiFPN and head models.\n",
    "            num_classes: an integer representing number of classes to predict.\n",
    "            num_anchors: an integer representing number of anchor boxes.\n",
    "            bifpn_depth: an integer representing number of BiFPN layers.\n",
    "            bifpn_kernel_size: an integer or tuple/list of 2 integers, specifying\n",
    "                the height and width of the 2D convolution window for BiFPN layers.\n",
    "            bifpn_depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers in BiFPN nodes.\n",
    "            bifpn_pooling_strategy: a string representing pooling strategy in BiFPN\n",
    "                layers. 'avg' or 'max'. Otherwise the max pooling will be selected.\n",
    "            heads_depth: an integer representing number of separable convolutions\n",
    "                before final convolution in head models.\n",
    "            class_kernel_size: an integer or tuple/list of 2 integers, specifying\n",
    "                the height and width of the 2D convolution window for\n",
    "                classifier model.\n",
    "            class_depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers in classifier model.\n",
    "            box_kernel_size: an integer or tuple/list of 2 integers, specifying\n",
    "                the height and width of the 2D convolution window for\n",
    "                regression model.\n",
    "            box_depth_multiplier: an integer representing depth multiplier for\n",
    "                separable convolution layers in regression model.\n",
    "            name: a string representing model name.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.num_classes=num_classes\n",
    "\n",
    "        self.backbone = get_backbone(backbone_name)\n",
    "        self.backbone.trainable = False\n",
    "\n",
    "        self.BiFPN = BiFPN(channels=channels,\n",
    "                           depth=bifpn_depth,\n",
    "                           kernel_size=bifpn_kernel_size,\n",
    "                           depth_multiplier=bifpn_depth_multiplier,\n",
    "                           pooling_strategy=bifpn_pooling_strategy)\n",
    "        self.class_det = ClassDetector(channels=channels,\n",
    "                                       num_classes=num_classes,\n",
    "                                       num_anchors=num_anchors,\n",
    "                                       depth=heads_depth,\n",
    "                                       kernel_size=class_kernel_size,\n",
    "                                       depth_multiplier=class_depth_multiplier)\n",
    "        self.box_reg = BoxRegressor(channels=channels,\n",
    "                                    num_anchors=num_anchors,\n",
    "                                    depth=heads_depth,\n",
    "                                    kernel_size=box_kernel_size,\n",
    "                                    depth_multiplier=box_depth_multiplier)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        features = self.backbone(inputs)\n",
    "        features.append(tf.keras.layers.AveragePooling2D()(features[-1]))\n",
    "        features.append(tf.keras.layers.AveragePooling2D()(features[-1]))\n",
    "\n",
    "        fpn_features = self.BiFPN(features, training=training)\n",
    "\n",
    "        classes = list()\n",
    "        boxes = list()\n",
    "        for feature in fpn_features:\n",
    "            classes.append(tf.reshape(self.class_det(feature, training=training), [batch_size, -1, self.num_classes]))\n",
    "            boxes.append(tf.reshape(self.box_reg(feature, training=training), [batch_size, -1, 4]))\n",
    "\n",
    "        classes = tf.concat(classes, axis=1)\n",
    "        boxes = tf.concat(boxes, axis=1)\n",
    "\n",
    "        return tf.concat([boxes, classes], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficientdet(name='efficientdet_d0',\n",
    "                     num_classes=80,\n",
    "                     num_anchors=9):\n",
    "    models = {\n",
    "        'efficientdet_d0': (64, 3, 3, 'efficientnet_b0'),\n",
    "        'efficientdet_d1': (88, 4, 3, 'efficientnet_b1'),\n",
    "        'efficientdet_d2': (112, 5, 3, 'efficientnet_b2'),\n",
    "        'efficientdet_d3': (160, 6, 4, 'efficientnet_b3'),\n",
    "        'efficientdet_d4': (224, 7, 4, 'efficientnet_b4'),\n",
    "        'efficientdet_d5': (288, 7, 4, 'efficientnet_b5'),\n",
    "        'efficientdet_d6': (384, 8, 5, 'efficientnet_b6'),\n",
    "        'efficientdet_d7': (384, 8, 5, 'efficientnet_b6'),\n",
    "    }\n",
    "    return EfficientDet(channels=models[name][0],\n",
    "                        num_classes=num_classes,\n",
    "                        num_anchors=num_anchors,\n",
    "                        bifpn_depth=models[name][1],\n",
    "                        heads_depth=models[name][2],\n",
    "                        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_efficientdet(MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "loss = EffDetLoss(num_classes=1)\n",
    "\n",
    "LR = tf.keras.experimental.CosineDecay(INITIAL_LR, DECAY_STEPS, 1e-3)\n",
    "opt = tf.keras.optimizers.SGD(LR, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
